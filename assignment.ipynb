{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyYaroWfELFo"
   },
   "source": [
    "# Programming Assignment\n",
    "\n",
    "**Warning**:\n",
    "\n",
    "Please modify only the code that is between\n",
    "```Python\n",
    "### YOUR CODE HERE ###\n",
    "```\n",
    "and\n",
    "```Python\n",
    "### ^^^^^^^^^^^^^^ ###\n",
    "```\n",
    "Some of the cells to be completed by the student are followed by a \"check\" cell that performs some basic unit tests. Run them after completing the code to check your results. Their only purpose is to help the student and will not be used to evaluate the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting keras\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow[and-cuda]\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting numpy>=1.19.5 (from scikit-learn)\n",
      "  Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting absl-py (from keras)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting h5py (from keras)\n",
      "  Downloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging in /home/vscode/.local/lib/python3.12/site-packages (from keras) (24.1)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow[and-cuda])\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow[and-cuda])\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow[and-cuda])\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow[and-cuda])\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow[and-cuda])\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow[and-cuda])\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow[and-cuda])\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow[and-cuda])\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow[and-cuda])\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow[and-cuda])\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow[and-cuda])\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow[and-cuda])\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow[and-cuda])\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting numpy>=1.19.5 (from scikit-learn)\n",
      "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting nvidia-cublas-cu12==12.5.3.2 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cublas_cu12-12.5.3.2-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvcc-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvcc_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.3.0.75 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cudnn_cu12-9.3.0.75-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.3.61 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cufft_cu12-11.2.3.61-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.6.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_curand_cu12-10.3.6.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.3.83 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusolver_cu12-11.6.3.83-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.1.3 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusparse_cu12-12.5.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.5.82 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.43.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow[and-cuda])\n",
      "  Downloading charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow[and-cuda])\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow[and-cuda])\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow[and-cuda])\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow[and-cuda])\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow[and-cuda])\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow[and-cuda])\n",
      "  Downloading werkzeug-3.1.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vscode/.local/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow[and-cuda])\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.5.3.2-py3-none-manylinux2014_x86_64.whl (363.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.3/363.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (22.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (24.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (895 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.7/895.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.3.0.75-py3-none-manylinux2014_x86_64.whl (577.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.2/577.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.3.61-py3-none-manylinux2014_x86_64.whl (192.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.6.82-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.3.83-py3-none-manylinux2014_x86_64.whl (130.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.1.3-py3-none-manylinux2014_x86_64.whl (217.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.6/217.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (362 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:02\u001b[0mm\n",
      "\u001b[?25hDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.2-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, urllib3, typing-extensions, threadpoolctl, termcolor, tensorboard-data-server, protobuf, opt-einsum, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, mdurl, MarkupSafe, markdown, joblib, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, astunparse, absl-py, werkzeug, scipy, requests, optree, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, ml-dtypes, markdown-it-py, h5py, tensorboard, scikit-learn, rich, nvidia-cusolver-cu12, keras, tensorflow\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.1.0 astunparse-1.6.3 certifi-2024.8.30 charset-normalizer-3.4.0 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.67.1 h5py-3.12.1 idna-3.10 joblib-1.4.2 keras-3.6.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 nvidia-cublas-cu12-12.5.3.2 nvidia-cuda-cupti-cu12-12.5.82 nvidia-cuda-nvcc-cu12-12.5.82 nvidia-cuda-nvrtc-cu12-12.5.82 nvidia-cuda-runtime-cu12-12.5.82 nvidia-cudnn-cu12-9.3.0.75 nvidia-cufft-cu12-11.2.3.61 nvidia-curand-cu12-10.3.6.82 nvidia-cusolver-cu12-11.6.3.83 nvidia-cusparse-cu12-12.5.1.3 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.5.82 opt-einsum-3.4.0 optree-0.13.0 protobuf-5.28.3 requests-2.32.3 rich-13.9.4 scikit-learn-1.5.2 scipy-1.14.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0 threadpoolctl-3.5.0 typing-extensions-4.12.2 urllib3-2.2.3 werkzeug-3.1.2 wrapt-1.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn keras tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a basic BoW model for a text classification problem with the [20 newsgroups](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "X, y = fetch_20newsgroups(return_X_y=True, subset=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn, create a pipeline consisting of a TF-IDF vectorization with 1-, 2- and 3-grams, and a Random Forest with 100 trees of max depth of 4. Evaluate it's f1-macro score with a 3-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-macro: 0.4913328964708052\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 3))),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=4))\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "f1_macro = cross_val_score(pipeline, X, y, cv=cv, scoring='f1_macro')\n",
    "\n",
    "print(f'f1-macro: {f1_macro.mean()}')\n",
    "### ^^^^^^^^^^^^^^ ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Notation\n",
    "\n",
    "- Subscript $n$ denotes the $n^{th}$ sample. \n",
    "\n",
    "- Superscript $[l]$ denotes the $l^{th}$ layer. \n",
    "\n",
    "- Superscript $\\langle t \\rangle$ denotes the $t^{th}$ time step. \n",
    "    \n",
    "- Subscript $(d)$ denotes the $d^{th}$ entry of a vector.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "$z^{[2]<3>}_{1(4)}$ denotes the activation of the 1st training sample, [2]nd layer, <3>rd time step, and (4)th entry in the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "03kGDM_sELFv"
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Recurrent Neural Network (RNN)\n",
    "\n",
    "### Input $x$\n",
    "\n",
    "* A training sample $x_n$ consists of multiple time steps $T_{x_n}$. Let's assume $T_x$ be the number of timesteps in the longest $x_n$.\n",
    "* A time step of an input sample, $x^{\\langle t \\rangle }_n$, is a one-dimensional input vector of size $n_x$. For example, a 1000-word vocabulary/dictionary would be encoded as a collection of one-hot vectors of size $n_x=D$ of 1000, and word $x_n^{\\langle t \\rangle}$ would have shape (1000,).\n",
    "* Let's use mini-batches of size $m$ of $x_n$ samples to benefit from vectorization. The shape of one such batch will be $(n_x,m,T_x)$. For example, if mini-batches of size 10 are used, the resulting 3D tensor will be (5000,20,10)-shaped.\n",
    "* For each time step $t$, a 2D slice $x^{\\langle t \\rangle}$ of shape $(n_x,m)$ will be fed into the network.\n",
    "\n",
    "### Hidden state $z$\n",
    "\n",
    "* The shape of a mini-batch of hidden states is $(n_z,m,T_x)$ including the time step dimension, with $n_z$ the number of hidden units.\n",
    "* $z^{\\langle t \\rangle}$ is a 2D slice at timestep $t$ of this minibatch, and has a shape of $(n_z, m)$.\n",
    "\n",
    "### Output $\\hat{y}$\n",
    "* $\\hat{y}$ is a 3D tensor of shape $(n_y,m,T_y)$, with $n_{y}$ the number of units in the vector representing the prediction and $T_{y}$ the number of time steps in the prediction. Lets assume $T_y = T_x$.\n",
    "* $\\hat{y}^{\\langle t \\rangle}$ is a 2D slice for time step $t$ and has a shape of $(n_{y}, m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oXWAKeTELF0"
   },
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - RNN Cell\n",
    "\n",
    "Let's start by implementing the computations for a single time step of the RNN cell.\n",
    "\n",
    "**Instructions**:\n",
    "1. Compute the hidden state: $z^{\\langle t \\rangle} = \\tanh(W_z z^{\\langle t-1 \\rangle} + W_x x^{\\langle t \\rangle} + b)$. Use [numpy.tanh](https://numpy.org/devdocs/reference/generated/numpy.tanh.html) and [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html).\n",
    "2. Use hidden state $z^{\\langle t \\rangle}$ to compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_y z^{\\langle t \\rangle} + b_y)$. Use the provided `softmax` function.\n",
    "4. Return $z^{\\langle t \\rangle}$ , $\\hat{y}^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxI-F0HWELF1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "  expon = np.exp(x)\n",
    "  return expon/np.sum(expon)\n",
    "\n",
    "def rnn_cell(x, z, Wx, Wy, Wz, b, by):\n",
    "    \"\"\"\n",
    "    Single step of the RNN cell.\n",
    "\n",
    "    Arguments:\n",
    "        x: input data at timestep \"t\", numpy array of shape (n_x,m).\n",
    "        z: hidden state at timestep \"t-1\", numpy array of shape (n_z,m)\n",
    "        Wx: weight matrix multiplying the input, numpy array of shape (n_z,n_x)\n",
    "        Wy: weight matrix relating the hidden-state to the output, numpy array of shape (n_y,n_z)\n",
    "        Wz: weight matrix multiplying the hidden state, numpy array of shape (n_z,n_z)\n",
    "        b: bias, numpy array of shape (n_z,1)\n",
    "        by: bias relating the hidden-state to the output, numpy array of shape (n_y,1)\n",
    "    Returns:\n",
    "        z: hidden state at timestep \"t\", of shape (n_z,m)\n",
    "        y_pred: prediction at timestep \"t\", numpy array of shape (n_y,m)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    z = np.tanh(np.dot(Wz, z) + np.dot(Wx, x) + b)\n",
    "    y_pred = softmax(np.dot(Wy, z) + by)\n",
    "    ### ^^^^^^^^^^^^^^ ###\n",
    "    return z, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V03ZGazVELF4"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10)\n",
    "z = np.random.randn(5, 10)\n",
    "Wx = np.random.randn(5, 3)\n",
    "Wz = np.random.randn(5, 5)\n",
    "Wy = np.random.randn(2, 5)\n",
    "b = np.random.randn(5, 1)\n",
    "by = np.random.randn(2, 1)\n",
    "z, y_pred = rnn_cell(x, z, Wx, Wy, Wz, b, by)\n",
    "\n",
    "np.testing.assert_equal(z.shape, (5, 10))\n",
    "expected_z4 = np.array([-0.97683053, 0.99993218, 0.9999085, 0.99720415, 0.96532437, 0.24615069, 0.89433073, 0.88371261, 0.40821569, -0.99446565])\n",
    "np.testing.assert_allclose(z[4], expected_z4, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10))\n",
    "expected_y_pred1 = np.array([0.00402888, 0.03824577, 0.03569951, 0.07060504, 0.01322519, 0.03128045, 0.02207267, 0.25945201, 0.03835635, 0.00209625])\n",
    "np.testing.assert_allclose(y_pred[1], expected_y_pred1, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rjz378-tELF7"
   },
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - RNN Forward Propagation \n",
    "\n",
    "A recurrent neural network can now be implemented as the repeated use of a cell. If the input sequence is $T_x$ time steps long, the RNN cell will be called $T_x$ times.\n",
    "\n",
    "- The cell takes two inputs at each time step:\n",
    "    - $z^{\\langle t-1 \\rangle}$: the hidden state from the previous time step\n",
    "    - $x^{\\langle t \\rangle}$: the current time step's input data\n",
    "- The cell produces two outputs at each time step:\n",
    "    - $z^{\\langle t \\rangle}$: the hidden state at this time step\n",
    "    - $\\hat{y}^{\\langle t \\rangle}$: the prediction for this time step\n",
    "- The weights and biases $(W_z, W_x, b)$ are re-used each time step \n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "At each time step $t$:\n",
    "1. Get $x^{\\langle t \\rangle}$, the 2D slice of $x$ at time step $t$.\n",
    "2. Update the 2D hidden state $z^{\\langle t \\rangle}$ and the prediction $\\hat{y}^{\\langle t \\rangle}$ using `rnn_cell`.\n",
    "3. Store the 2D hidden state in the 3D tensor $z$, at the $t^{th}$ position.\n",
    "4. Store the 2D $\\hat{y}^{\\langle t \\rangle}$ prediction in the 3D tensor $\\hat{y}$ at the $t^{th}$ position.\n",
    "5. Return the 3D tensors $z$ and $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VmeprGJpELF9"
   },
   "outputs": [],
   "source": [
    "def rnn_forward(x, z, Wx, Wy, Wz, b, by):\n",
    "    \"\"\"\n",
    "    Forward propagation of the RNN.\n",
    "\n",
    "    Arguments:\n",
    "        x: input data for every time-step, of shape (n_x,m,T_x).\n",
    "        z: initial hidden state, of shape (n_z,m)\n",
    "        Wx: weight matrix multiplying the input, numpy array of shape (n_z,n_x)\n",
    "        Wy: weight matrix relating the hidden-state to the output, numpy array of shape (n_y,n_z)\n",
    "        Wz: weight matrix multiplying the hidden state, numpy array of shape (n_z,n_z)\n",
    "        b: bias numpy array of shape (n_z,1)\n",
    "        by: bias relating the hidden-state to the output, numpy array of shape (n_y,1)\n",
    "    Returns:\n",
    "        z: hidden states for every time-step, numpy array of shape (n_z,m,T_x)\n",
    "        y_pred: predictions for every time-step, numpy array of shape (n_y,m,T_x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions and initialize z, y_pred and zt\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_z = Wy.shape\n",
    "    zt = z\n",
    "    z = np.zeros((n_z, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # loop over time steps\n",
    "    for t in range(T_x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Update the hidden state and compute the prediction with rnn_cell\n",
    "        zt, yt_pred = rnn_cell(x[:, :, t], zt, Wx, Wy, Wz, b, by)\n",
    "        # Save the value of the hidden state at time step t in z\n",
    "        z[:,:,t] = zt\n",
    "        # Save the value of the prediction at time step t in y_pred\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        ### ^^^^^^^^^^^^^^ ###\n",
    "    \n",
    "    return z, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEPrd77rELF_"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10, 4)\n",
    "z = np.random.randn(5, 10)\n",
    "Wz = np.random.randn(5, 5)\n",
    "Wx = np.random.randn(5, 3)\n",
    "Wy = np.random.randn(2, 5)\n",
    "b = np.random.randn(5, 1)\n",
    "by = np.random.randn(2, 1)\n",
    "z, y_pred = rnn_forward(x, z, Wx, Wy, Wz, b, by)\n",
    "\n",
    "np.testing.assert_equal(z.shape, (5, 10, 4))\n",
    "expected_z4 = np.array([-0.99999375, 0.77911235, -0.99861469, -0.99833267])\n",
    "np.testing.assert_allclose(z[4][1], expected_z4, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10, 4))\n",
    "expected_y_pred1 = np.array([0.01644489, 0.0328586, 0.0014877, 0.02851197])\n",
    "np.testing.assert_allclose(y_pred[1][3], expected_y_pred1, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2QbsWFzELGD"
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Long Short-Term Memory (LSTM)\n",
    "\n",
    "The RNN works best when each output $\\hat{y}^{\\langle t \\rangle}$ can be estimated using \"local\" context, but suffers from vanishing gradient and cannot remember information for many time steps. LSTM can cope with vanishing gradient much better.\n",
    "\n",
    "Let's begin by implementing the LSTM cell for a single time step. Then, it will be possible to call it from a loop to have it process an input with $T_x$ time steps.\n",
    "\n",
    "### Gates and states\n",
    "\n",
    "#### Forget gate $f$\n",
    "\n",
    "The forget gate is a tensor containing values between 0 and 1.\n",
    "    * If a unit in the forget gate has a value close to 0, the LSTM will \"forget\" the stored previous state in the corresponding unit.\n",
    "    * If a unit in the forget gate has a value close to 1, the LSTM will \"remember\" the the stored previous state in the corresponding unit.\n",
    "\n",
    "$$f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{h}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)$$\n",
    "\n",
    "#### Candidate cell value $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$\n",
    "* The candidate cell value is a tensor containing information from the current time step that may be stored in the current cell state $\\mathbf{c}^{\\langle t \\rangle}$.\n",
    "* The parts of the candidate cell value that get passed on depend on the input gate.\n",
    "* The candidate cell value is a tensor containing values that range from -1 to 1.\n",
    "\n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{h}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right)$$\n",
    "\n",
    "#### Input gate $i$\n",
    "\n",
    "* The input gate decides what entries of the candidate cell value $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ to add to the cell state $c^{\\langle t \\rangle}$.\n",
    "* The update gate is a tensor containing values between 0 and 1.\n",
    "    * When a unit in the update gate is close to 1, it allows the value of the candidate $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ to be passed onto the hidden state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "    * When a unit in the update gate is close to 0, it prevents the corresponding value in the candidate from being passed onto the hidden state.\n",
    "\n",
    "$$i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[h^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\\tag{2}$$\n",
    "\n",
    "#### Cell state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "\n",
    "* The cell state is the \"memory\" passed to future time steps.\n",
    "* The new cell state $\\mathbf{c}^{\\langle t \\rangle}$ is a combination of the previous cell state and the candidate value.\n",
    "\n",
    "$$\\mathbf{c}^{\\langle t \\rangle} = f^{\\langle t \\rangle}*\\mathbf{c}^{\\langle t-1 \\rangle} + i^{\\langle t \\rangle}*\\mathbf{\\tilde{c}}^{\\langle t \\rangle}$$\n",
    "\n",
    "#### Output gate $o$\n",
    "\n",
    "* The output gate decides what gets sent as the output (prediction) of the time step.\n",
    "* The output gate contains values that range from 0 to 1.\n",
    "\n",
    "$$o^{\\langle t \\rangle}= \\sigma(\\mathbf{W}_o[\\mathbf{h}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})$$ \n",
    "\n",
    "\n",
    "#### Hidden state $\\mathbf{h}^{\\langle t \\rangle}$\n",
    "\n",
    "* The hidden state gets passed to the LSTM cell's next time step.\n",
    "* It is used to determine the three gates ($f, i, o$) of the next time step.\n",
    "* The hidden state is also used for the prediction $y^{\\langle t \\rangle}$.\n",
    "\n",
    "$$ \\mathbf{h}^{\\langle t \\rangle} = o^{\\langle t \\rangle}*\\tanh(\\mathbf{c}^{\\langle t \\rangle})$$\n",
    "\n",
    "\n",
    "#### Prediction $\\mathbf{y}^{\\langle t \\rangle}_{pred}$\n",
    "\n",
    "* The output or prediction.\n",
    "\n",
    "If the problem is multiclass classification (therefore with a softmax activation at the output), for example, the equation would be:\n",
    "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{h}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G49sqmnoELGI"
   },
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - LSTM Cell\n",
    "\n",
    "Lets implement the LSTM cell.\n",
    "\n",
    "**Instructions**:\n",
    "1. Compute all formulas for the gates and states. Use [numpy.tanh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html) and [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) and the provided functions `sigmoid` and `softmax`.\n",
    "2. Compute the prediction $y^{\\langle t \\rangle}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JU3tUxvmELGJ"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1/(1 + np.exp(-x))\n",
    "\n",
    "def lstm_cell(x, h, c, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by):\n",
    "    \"\"\"\n",
    "    Single step of the LSTM cell.\n",
    "\n",
    "    Arguments:\n",
    "        x: your input data at timestep \"t\", numpy array of shape (n_x,m).\n",
    "        h: hidden state at timestep \"t-1\", numpy array of shape (n_h,m).\n",
    "        c: memory state at timestep \"t-1\", numpy array of shape (n_h,m).\n",
    "        Wf: weight matrix of the forget gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bf: bias of the forget gate, numpy array of shape (n_h,1).\n",
    "        Wi: weight matrix of the input gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bi: bias of the input gate, numpy array of shape (n_h,1).\n",
    "        Wc: weight matrix of the first \"tanh\", numpy array of shape (n_h,n_h + n_x).\n",
    "        bc: bias of the first \"tanh\", numpy array of shape (n_h,1).\n",
    "        Wo: weight matrix of the output gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bo: bias of the output gate, numpy array of shape (n_h,1).\n",
    "        Wy: weight matrix relating the hidden state to the output, numpy array of shape (n_y,n_h).\n",
    "        by: bias relating the hidden state to the output, numpy array of shape (n_y,1).                    \n",
    "    Returns:\n",
    "        h_next: next hidden state, of shape (n_h,m).\n",
    "        c_next: next memory state, of shape (n_h,m).\n",
    "        yt_pred: prediction at timestep \"t\", numpy array of shape (n_y,m).\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve dimensions from shapes of x and Wy and concatenate h and x\n",
    "    n_x, m = x.shape\n",
    "    n_y, n_h = Wy.shape\n",
    "    concat = np.concatenate((h, x))\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    # Compute values for f, i, cct, c, o, h using the formulas\n",
    "    f = sigmoid(np.dot(Wf, concat) + bf)\n",
    "    i = sigmoid(np.dot(Wi, concat) + bi)\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)\n",
    "    c = f * c + i * cct\n",
    "    o = sigmoid(np.dot(Wo, concat) + bo)\n",
    "    h = o * np.tanh(c)\n",
    "\n",
    "    # Compute prediction of the LSTM cell\n",
    "    y_pred = softmax(np.dot(Wy, h) + by)\n",
    "    ### ^^^^^^^^^^^^^^ ###\n",
    "\n",
    "    return h, c, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9ssBEoxELGN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10)\n",
    "h = np.random.randn(5, 10)\n",
    "c = np.random.randn(5, 10)\n",
    "Wf = np.random.randn(5, 5 + 3)\n",
    "bf = np.random.randn(5, 1)\n",
    "Wi = np.random.randn(5, 5 + 3)\n",
    "bi = np.random.randn(5, 1)\n",
    "Wo = np.random.randn(5, 5 + 3)\n",
    "bo = np.random.randn(5, 1)\n",
    "Wc = np.random.randn(5, 5 + 3)\n",
    "bc = np.random.randn(5, 1)\n",
    "Wy = np.random.randn(2, 5)\n",
    "by = np.random.randn(2, 1)\n",
    "h, c, y_pred = lstm_cell(x, h, c, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by)\n",
    "\n",
    "np.testing.assert_equal(h.shape, (5, 10))\n",
    "expected_h4 = np.array([-0.66408471, 0.0036921, 0.02088357, 0.22834167, -0.85575339, 0.00138482, 0.76566531, 0.34631421, -0.00215674, 0.43827275])\n",
    "np.testing.assert_allclose(h[4], expected_h4, rtol=1e-2)\n",
    "np.testing.assert_equal(c.shape, (5, 10))\n",
    "expected_c2 = np.array([0.63267805, 1.00570849, 0.35504474, 0.20690913, -1.64566718, 0.11832942, 0.76449811, -0.0981561, -0.74348425, -0.26810932])\n",
    "np.testing.assert_allclose(c[2], expected_c2, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10))\n",
    "expected_y_pred1 = np.array([0.0283805, 0.00706782, 0.00753306, 0.01091349, 0.02806865, 0.00955958, 0.00444923, 0.01152125, 0.01426974, 0.01237766])\n",
    "np.testing.assert_allclose(y_pred[1], expected_y_pred1, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tb-4WWn4ELGQ"
   },
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - LSTM Forward Propagation\n",
    "\n",
    "Now it is possible to iterate over the LSTM cell using a for loop to process a sequence of $T_x$ inputs.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "For each time step:\n",
    "1. From the 3D tensor $x$, get a 2D slice $x^{\\langle t \\rangle}$ at time step $t$.\n",
    "2. Call the `lstm_cell` function to get the hidden state, cell state and prediction.\n",
    "3. Store the hidden state, cell state and prediction (the 2D tensors) inside the 3D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XMmJrPSdELGQ"
   },
   "outputs": [],
   "source": [
    "def lstm_forward(x, h, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by):\n",
    "    \"\"\"\n",
    "    Forward propagation of the LSTM network.\n",
    "\n",
    "    Arguments:\n",
    "        x: input data for every time step, of shape (n_x,m,T_x).\n",
    "        h: initial hidden state, of shape (n_h,m).\n",
    "        Wf: weight matrix of the forget gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bf: bias of the forget gate, numpy array of shape (n_h,1).\n",
    "        Wi: weight matrix of the input gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bi: bias of the input gate, numpy array of shape (n_h,1).\n",
    "        Wc: weight matrix of the first \"tanh\", numpy array of shape (n_h,n_h + n_x).\n",
    "        bc: bias of the first \"tanh\", numpy array of shape (n_h,1).\n",
    "        Wo: weight matrix of the output gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bo: bias of the output gate, numpy array of shape (n_h,1).\n",
    "        Wy: weight matrix relating the hidden state to the output, numpy array of shape (n_y,n_h).\n",
    "        by: bias relating the hidden state to the output, numpy array of shape (n_y,1).\n",
    "    Returns:\n",
    "        a: hidden states for every time step, numpy array of shape (n_h,m,T_x).\n",
    "        y: predictions for every time step, numpy array of shape (n_y,m,T_x).\n",
    "        c: cell state, numpy array of shape (n_h,m,T_x).\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve dimensions from shapes of x and Wy and initialize h, c and y\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_h = Wy.shape\n",
    "    ht = h\n",
    "    ct = np.zeros((n_h, m))\n",
    "    h = np.zeros((n_h, m, T_x))\n",
    "    c = np.zeros((n_h, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # loop over time steps\n",
    "    for t in range(T_x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n",
    "        xt = x[:, :, t]\n",
    "        # Update next hidden state and next memory state and compute the prediction with lstm_cell\n",
    "        ht, ct, yt_pred = lstm_cell(xt, ht, ct, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by)\n",
    "        # Save the value of the new \"next\" hidden state in h\n",
    "        h[:, :, t] = ht\n",
    "        # Save the value of the next cell state in c\n",
    "        c[:, :, t] = ct\n",
    "        # Save the value of the prediction in y_pred\n",
    "        y_pred[:, :, t] = yt_pred\n",
    "        ### ^^^^^^^^^^^^^^ ###\n",
    "    \n",
    "    return h, c, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JehC5gwdELGS"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10, 7)\n",
    "h = np.random.randn(5, 10)\n",
    "Wf = np.random.randn(5, 5 + 3)\n",
    "bf = np.random.randn(5, 1)\n",
    "Wi = np.random.randn(5, 5 + 3)\n",
    "bi= np.random.randn(5, 1)\n",
    "Wo = np.random.randn(5, 5 + 3)\n",
    "bo = np.random.randn(5, 1)\n",
    "Wc = np.random.randn(5, 5 + 3)\n",
    "bc = np.random.randn(5, 1)\n",
    "Wy = np.random.randn(2, 5)\n",
    "by = np.random.randn(2, 1)\n",
    "h, c, y_pred = lstm_forward(x, h, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by)\n",
    "\n",
    "np.testing.assert_equal(h.shape, (5, 10, 7))\n",
    "expected_h436 = 0.17211776753291672\n",
    "np.testing.assert_allclose(h[4][3][6], expected_h436, rtol=1e-2)\n",
    "np.testing.assert_equal(c.shape, (5, 10, 7))\n",
    "expected_c121 = -0.8555449167181981\n",
    "np.testing.assert_allclose(c[1][2][1], expected_c121, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10, 7))\n",
    "expected_y_pred143 = 0.10837052997887556\n",
    "np.testing.assert_allclose(y_pred[1][4][3], expected_y_pred143, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's build a new classifier, this time for the IMDB dataset, using a Transformer Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 12:43:18.705913: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-05 12:43:18.713545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730810598.721590    1136 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730810598.723856    1136 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-05 12:43:18.733002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "X_train = keras.utils.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = keras.utils.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using keras, implement a transformer block and a token and position embedding as layers, and use them to build a classifier. Train it for 1 epoch with Adam on the training partition while using the test partition to calculate the validation loss and accuracy at every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730810604.566534    1136 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730810610.047238    2744 service.cc:148] XLA service 0x7f010800c670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1730810610.047268    2744 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2024-11-05 12:43:30.169081: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1730810610.698838    2744 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-05 12:43:32.069943: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_145', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:32.122789: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_91', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:32.139724: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_145', 708 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:32.558942: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:32.615425: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:32.631681: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:32.647403: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 160 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:32.663209: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_145', 176 bytes spill stores, 208 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:32.973428: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 996 bytes spill stores, 768 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:33.041167: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:33.229720: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 40 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:33.259384: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 716 bytes spill stores, 740 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:33.456180: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 388 bytes spill stores, 356 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:33.547179: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:34.107207: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_156', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:34.120282: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9851', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:34.170828: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 176 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:34.197783: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:34.518744: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 788 bytes spill stores, 812 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:34.665106: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28_0', 332 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:34.919416: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9710', 1056 bytes spill stores, 996 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:34.920407: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_156', 988 bytes spill stores, 760 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:34.971280: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:35.176170: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_156', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:35.214999: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 88 bytes spill stores, 88 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:35.451674: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9710', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:35.755256: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9710', 744 bytes spill stores, 768 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:35.803683: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9710', 20 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-05 12:43:40.993388: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'copy_fusion', 608 bytes spill stores, 608 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 52 bytes spill stores, 52 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_add_reduce_select_fusion_2', 44 bytes spill stores, 44 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_add_reduce_select_fusion_1', 44 bytes spill stores, 44 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_add_reduce_select_fusion', 100 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "I0000 00:00:1730810621.053574    2744 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5003 - loss: 0.7782"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 12:44:06.463871: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 80 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:06.534274: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:06.562751: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 208 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:06.588029: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:06.614197: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 708 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:06.618102: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 996 bytes spill stores, 768 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:06.870010: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 384 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:07.027779: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 40 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:07.043977: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:07.112921: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 176 bytes spill stores, 208 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:07.120193: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:07.186863: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:07.485600: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 160 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:07.703358: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_163', 176 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:07.742059: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:08.057240: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_161', 716 bytes spill stores, 740 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:08.120439: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:08.125052: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_161', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:08.489059: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:08.505003: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_164', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:08.633035: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_161', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:08.735214: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:08.847620: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_163', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:08.894357: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_164', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:09.458941: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 1176 bytes spill stores, 1160 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:09.628252: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_163', 788 bytes spill stores, 812 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:09.844855: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_164', 988 bytes spill stores, 760 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:09.898334: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 740 bytes spill stores, 764 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:09.930975: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:10.221143: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:10.655822: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28_0', 2112 bytes spill stores, 2880 bytes spill loads\n",
      "\n",
      "2024-11-05 12:44:16.598716: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_10', 384 bytes spill stores, 384 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 8 bytes spill stores, 8 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_add_multiply_reduce_select_fusion', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 58ms/step - accuracy: 0.5003 - loss: 0.7780 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.5030 - loss: 0.6966 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.4996 - loss: 0.6969 - val_accuracy: 0.5000 - val_loss: 0.6950\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.4972 - loss: 0.6956 - val_accuracy: 0.5000 - val_loss: 0.6940\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - accuracy: 0.5027 - loss: 0.6950 - val_accuracy: 0.5000 - val_loss: 0.6954\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - accuracy: 0.4937 - loss: 0.6948 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - accuracy: 0.4933 - loss: 0.6939 - val_accuracy: 0.5000 - val_loss: 0.6933\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - accuracy: 0.4956 - loss: 0.6940 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - accuracy: 0.4971 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6945\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - accuracy: 0.5081 - loss: 0.6933 - val_accuracy: 0.5000 - val_loss: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f0263a68170>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, Dropout\n",
    "from keras.layers import LayerNormalization, MultiHeadAttention, Add\n",
    "def PositionalEncoding(maxlen, model_dim):\n",
    "    def _positional_encoding(inputs):\n",
    "        # Get the shape of the inputs\n",
    "        _, maxlen, model_dim = inputs.shape\n",
    "        # Create position and dimension indices\n",
    "        pos = np.arange(0, maxlen).reshape(-1, 1)\n",
    "        i = np.arange(0, model_dim).reshape(1, -1)\n",
    "        # Calculate the angle rates\n",
    "        angle_rads = pos / np.power(10000, (2 * (i // 2) / model_dim))\n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        # Add the positional encoding to the inputs\n",
    "        angle_rads = angle_rads[np.newaxis, ...]\n",
    "        return inputs + angle_rads\n",
    "    return keras.layers.Lambda(_positional_encoding)\n",
    "\n",
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = LayerNormalization()(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    # Residual and Feed Forward\n",
    "    x = Add()([inputs, x])\n",
    "    y = LayerNormalization()(x)\n",
    "    x = Dense(ff_dim, activation='relu')(y)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    return Add()([y, x])\n",
    "\n",
    "# Define hyperparameters\n",
    "head_size = 256\n",
    "num_heads = 4\n",
    "ff_dim = 4\n",
    "dropout = 0.3\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(maxlen,))\n",
    "\n",
    "# Embedding layer\n",
    "embedding = Embedding(vocab_size, 128)(inputs)\n",
    "\n",
    "# Add positional encoding to the embedding\n",
    "x = embedding + PositionalEncoding(maxlen, 128)(embedding)\n",
    "\n",
    "# Add transformer blocks\n",
    "for _ in range(4):\n",
    "    x = transformer_block(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "# Global average pooling\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(dropout)(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "### ^^^^^^^^^^^^^^ ###"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "DLSC5W1-A1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
