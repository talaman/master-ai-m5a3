{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyYaroWfELFo"
   },
   "source": [
    "# Programming Assignment\n",
    "\n",
    "**Warning**:\n",
    "\n",
    "Please modify only the code that is between\n",
    "```Python\n",
    "### YOUR CODE HERE ###\n",
    "```\n",
    "and\n",
    "```Python\n",
    "### ^^^^^^^^^^^^^^ ###\n",
    "```\n",
    "Some of the cells to be completed by the student are followed by a \"check\" cell that performs some basic unit tests. Run them after completing the code to check your results. Their only purpose is to help the student and will not be used to evaluate the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a basic BoW model for a text classification problem with the [20 newsgroups](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "X, y = fetch_20newsgroups(return_X_y=True, subset=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn, create a pipeline consisting of a TF-IDF vectorization with 1-, 2- and 3-grams, and a Random Forest with 100 trees of max depth of 4. Evaluate it's f1-macro score with a 3-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "...\n",
    "### ^^^^^^^^^^^^^^ ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Notation\n",
    "\n",
    "- Subscript $n$ denotes the $n^{th}$ sample. \n",
    "\n",
    "- Superscript $[l]$ denotes the $l^{th}$ layer. \n",
    "\n",
    "- Superscript $\\langle t \\rangle$ denotes the $t^{th}$ time step. \n",
    "    \n",
    "- Subscript $(d)$ denotes the $d^{th}$ entry of a vector.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "$z^{[2]<3>}_{1(4)}$ denotes the activation of the 1st training sample, [2]nd layer, <3>rd time step, and (4)th entry in the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "03kGDM_sELFv"
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Recurrent Neural Network (RNN)\n",
    "\n",
    "### Input $x$\n",
    "\n",
    "* A training sample $x_n$ consists of multiple time steps $T_{x_n}$. Let's assume $T_x$ be the number of timesteps in the longest $x_n$.\n",
    "* A time step of an input sample, $x^{\\langle t \\rangle }_n$, is a one-dimensional input vector of size $n_x$. For example, a 1000-word vocabulary/dictionary would be encoded as a collection of one-hot vectors of size $n_x=D$ of 1000, and word $x_n^{\\langle t \\rangle}$ would have shape (1000,).\n",
    "* Let's use mini-batches of size $m$ of $x_n$ samples to benefit from vectorization. The shape of one such batch will be $(n_x,m,T_x)$. For example, if mini-batches of size 10 are used, the resulting 3D tensor will be (5000,20,10)-shaped.\n",
    "* For each time step $t$, a 2D slice $x^{\\langle t \\rangle}$ of shape $(n_x,m)$ will be fed into the network.\n",
    "\n",
    "### Hidden state $z$\n",
    "\n",
    "* The shape of a mini-batch of hidden states is $(n_z,m,T_x)$ including the time step dimension, with $n_z$ the number of hidden units.\n",
    "* $z^{\\langle t \\rangle}$ is a 2D slice at timestep $t$ of this minibatch, and has a shape of $(n_z, m)$.\n",
    "\n",
    "### Output $\\hat{y}$\n",
    "* $\\hat{y}$ is a 3D tensor of shape $(n_y,m,T_y)$, with $n_{y}$ the number of units in the vector representing the prediction and $T_{y}$ the number of time steps in the prediction. Lets assume $T_y = T_x$.\n",
    "* $\\hat{y}^{\\langle t \\rangle}$ is a 2D slice for time step $t$ and has a shape of $(n_{y}, m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oXWAKeTELF0"
   },
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - RNN Cell\n",
    "\n",
    "Let's start by implementing the computations for a single time step of the RNN cell.\n",
    "\n",
    "**Instructions**:\n",
    "1. Compute the hidden state: $z^{\\langle t \\rangle} = \\tanh(W_z z^{\\langle t-1 \\rangle} + W_x x^{\\langle t \\rangle} + b)$. Use [numpy.tanh](https://numpy.org/devdocs/reference/generated/numpy.tanh.html) and [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html).\n",
    "2. Use hidden state $z^{\\langle t \\rangle}$ to compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_y z^{\\langle t \\rangle} + b_y)$. Use the provided `softmax` function.\n",
    "4. Return $z^{\\langle t \\rangle}$ , $\\hat{y}^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxI-F0HWELF1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "  expon = np.exp(x)\n",
    "  return expon/np.sum(expon)\n",
    "\n",
    "def rnn_cell(x, z, Wx, Wy, Wz, b, by):\n",
    "    \"\"\"\n",
    "    Single step of the RNN cell.\n",
    "\n",
    "    Arguments:\n",
    "        x: input data at timestep \"t\", numpy array of shape (n_x,m).\n",
    "        z: hidden state at timestep \"t-1\", numpy array of shape (n_z,m)\n",
    "        Wx: weight matrix multiplying the input, numpy array of shape (n_z,n_x)\n",
    "        Wy: weight matrix relating the hidden-state to the output, numpy array of shape (n_y,n_z)\n",
    "        Wz: weight matrix multiplying the hidden state, numpy array of shape (n_z,n_z)\n",
    "        b: bias, numpy array of shape (n_z,1)\n",
    "        by: bias relating the hidden-state to the output, numpy array of shape (n_y,1)\n",
    "    Returns:\n",
    "        z: hidden state at timestep \"t\", of shape (n_z,m)\n",
    "        y_pred: prediction at timestep \"t\", numpy array of shape (n_y,m)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    z = ...\n",
    "    y_pred = ...\n",
    "    ### ^^^^^^^^^^^^^^ ###\n",
    "    return z, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V03ZGazVELF4"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10)\n",
    "z = np.random.randn(5, 10)\n",
    "Wx = np.random.randn(5, 3)\n",
    "Wz = np.random.randn(5, 5)\n",
    "Wy = np.random.randn(2, 5)\n",
    "b = np.random.randn(5, 1)\n",
    "by = np.random.randn(2, 1)\n",
    "z, y_pred = rnn_cell(x, z, Wx, Wy, Wz, b, by)\n",
    "\n",
    "np.testing.assert_equal(z.shape, (5, 10))\n",
    "expected_z4 = np.array([-0.97683053, 0.99993218, 0.9999085, 0.99720415, 0.96532437, 0.24615069, 0.89433073, 0.88371261, 0.40821569, -0.99446565])\n",
    "np.testing.assert_allclose(z[4], expected_z4, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10))\n",
    "expected_y_pred1 = np.array([0.00402888, 0.03824577, 0.03569951, 0.07060504, 0.01322519, 0.03128045, 0.02207267, 0.25945201, 0.03835635, 0.00209625])\n",
    "np.testing.assert_allclose(y_pred[1], expected_y_pred1, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rjz378-tELF7"
   },
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - RNN Forward Propagation \n",
    "\n",
    "A recurrent neural network can now be implemented as the repeated use of a cell. If the input sequence is $T_x$ time steps long, the RNN cell will be called $T_x$ times.\n",
    "\n",
    "- The cell takes two inputs at each time step:\n",
    "    - $z^{\\langle t-1 \\rangle}$: the hidden state from the previous time step\n",
    "    - $x^{\\langle t \\rangle}$: the current time step's input data\n",
    "- The cell produces two outputs at each time step:\n",
    "    - $z^{\\langle t \\rangle}$: the hidden state at this time step\n",
    "    - $\\hat{y}^{\\langle t \\rangle}$: the prediction for this time step\n",
    "- The weights and biases $(W_z, W_x, b)$ are re-used each time step \n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "At each time step $t$:\n",
    "1. Get $x^{\\langle t \\rangle}$, the 2D slice of $x$ at time step $t$.\n",
    "2. Update the 2D hidden state $z^{\\langle t \\rangle}$ and the prediction $\\hat{y}^{\\langle t \\rangle}$ using `rnn_cell`.\n",
    "3. Store the 2D hidden state in the 3D tensor $z$, at the $t^{th}$ position.\n",
    "4. Store the 2D $\\hat{y}^{\\langle t \\rangle}$ prediction in the 3D tensor $\\hat{y}$ at the $t^{th}$ position.\n",
    "5. Return the 3D tensors $z$ and $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VmeprGJpELF9"
   },
   "outputs": [],
   "source": [
    "def rnn_forward(x, z, Wx, Wy, Wz, b, by):\n",
    "    \"\"\"\n",
    "    Forward propagation of the RNN.\n",
    "\n",
    "    Arguments:\n",
    "        x: input data for every time-step, of shape (n_x,m,T_x).\n",
    "        z: initial hidden state, of shape (n_z,m)\n",
    "        Wx: weight matrix multiplying the input, numpy array of shape (n_z,n_x)\n",
    "        Wy: weight matrix relating the hidden-state to the output, numpy array of shape (n_y,n_z)\n",
    "        Wz: weight matrix multiplying the hidden state, numpy array of shape (n_z,n_z)\n",
    "        b: bias numpy array of shape (n_z,1)\n",
    "        by: bias relating the hidden-state to the output, numpy array of shape (n_y,1)\n",
    "    Returns:\n",
    "        z: hidden states for every time-step, numpy array of shape (n_z,m,T_x)\n",
    "        y_pred: predictions for every time-step, numpy array of shape (n_y,m,T_x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions and initialize z, y_pred and zt\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_z = Wy.shape\n",
    "    zt = z\n",
    "    z = np.zeros((n_z, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # loop over time steps\n",
    "    for t in range(T_x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Update the hidden state and compute the prediction with rnn_cell\n",
    "        zt, yt_pred = ...\n",
    "        # Save the value of the hidden state at time step t in z\n",
    "        z[:,:,t] = ...\n",
    "        # Save the value of the prediction at time step t in y_pred\n",
    "        y_pred[:,:,t] = ...\n",
    "        ### ^^^^^^^^^^^^^^ ###\n",
    "    \n",
    "    return z, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEPrd77rELF_"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10, 4)\n",
    "z = np.random.randn(5, 10)\n",
    "Wz = np.random.randn(5, 5)\n",
    "Wx = np.random.randn(5, 3)\n",
    "Wy = np.random.randn(2, 5)\n",
    "b = np.random.randn(5, 1)\n",
    "by = np.random.randn(2, 1)\n",
    "z, y_pred = rnn_forward(x, z, Wx, Wy, Wz, b, by)\n",
    "\n",
    "np.testing.assert_equal(z.shape, (5, 10, 4))\n",
    "expected_z4 = np.array([-0.99999375, 0.77911235, -0.99861469, -0.99833267])\n",
    "np.testing.assert_allclose(z[4][1], expected_z4, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10, 4))\n",
    "expected_y_pred1 = np.array([0.01644489, 0.0328586, 0.0014877, 0.02851197])\n",
    "np.testing.assert_allclose(y_pred[1][3], expected_y_pred1, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2QbsWFzELGD"
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Long Short-Term Memory (LSTM)\n",
    "\n",
    "The RNN works best when each output $\\hat{y}^{\\langle t \\rangle}$ can be estimated using \"local\" context, but suffers from vanishing gradient and cannot remember information for many time steps. LSTM can cope with vanishing gradient much better.\n",
    "\n",
    "Let's begin by implementing the LSTM cell for a single time step. Then, it will be possible to call it from a loop to have it process an input with $T_x$ time steps.\n",
    "\n",
    "### Gates and states\n",
    "\n",
    "#### Forget gate $f$\n",
    "\n",
    "The forget gate is a tensor containing values between 0 and 1.\n",
    "    * If a unit in the forget gate has a value close to 0, the LSTM will \"forget\" the stored previous state in the corresponding unit.\n",
    "    * If a unit in the forget gate has a value close to 1, the LSTM will \"remember\" the the stored previous state in the corresponding unit.\n",
    "\n",
    "$$f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{h}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)$$\n",
    "\n",
    "#### Candidate cell value $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$\n",
    "* The candidate cell value is a tensor containing information from the current time step that may be stored in the current cell state $\\mathbf{c}^{\\langle t \\rangle}$.\n",
    "* The parts of the candidate cell value that get passed on depend on the input gate.\n",
    "* The candidate cell value is a tensor containing values that range from -1 to 1.\n",
    "\n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{h}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right)$$\n",
    "\n",
    "#### Input gate $i$\n",
    "\n",
    "* The input gate decides what entries of the candidate cell value $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ to add to the cell state $c^{\\langle t \\rangle}$.\n",
    "* The update gate is a tensor containing values between 0 and 1.\n",
    "    * When a unit in the update gate is close to 1, it allows the value of the candidate $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ to be passed onto the hidden state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "    * When a unit in the update gate is close to 0, it prevents the corresponding value in the candidate from being passed onto the hidden state.\n",
    "\n",
    "$$i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[h^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\\tag{2}$$\n",
    "\n",
    "#### Cell state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "\n",
    "* The cell state is the \"memory\" passed to future time steps.\n",
    "* The new cell state $\\mathbf{c}^{\\langle t \\rangle}$ is a combination of the previous cell state and the candidate value.\n",
    "\n",
    "$$\\mathbf{c}^{\\langle t \\rangle} = f^{\\langle t \\rangle}*\\mathbf{c}^{\\langle t-1 \\rangle} + i^{\\langle t \\rangle}*\\mathbf{\\tilde{c}}^{\\langle t \\rangle}$$\n",
    "\n",
    "#### Output gate $o$\n",
    "\n",
    "* The output gate decides what gets sent as the output (prediction) of the time step.\n",
    "* The output gate contains values that range from 0 to 1.\n",
    "\n",
    "$$o^{\\langle t \\rangle}= \\sigma(\\mathbf{W}_o[\\mathbf{h}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})$$ \n",
    "\n",
    "\n",
    "#### Hidden state $\\mathbf{h}^{\\langle t \\rangle}$\n",
    "\n",
    "* The hidden state gets passed to the LSTM cell's next time step.\n",
    "* It is used to determine the three gates ($f, i, o$) of the next time step.\n",
    "* The hidden state is also used for the prediction $y^{\\langle t \\rangle}$.\n",
    "\n",
    "$$ \\mathbf{h}^{\\langle t \\rangle} = o^{\\langle t \\rangle}*\\tanh(\\mathbf{c}^{\\langle t \\rangle})$$\n",
    "\n",
    "\n",
    "#### Prediction $\\mathbf{y}^{\\langle t \\rangle}_{pred}$\n",
    "\n",
    "* The output or prediction.\n",
    "\n",
    "If the problem is multiclass classification (therefore with a softmax activation at the output), for example, the equation would be:\n",
    "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{h}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G49sqmnoELGI"
   },
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - LSTM Cell\n",
    "\n",
    "Lets implement the LSTM cell.\n",
    "\n",
    "**Instructions**:\n",
    "1. Compute all formulas for the gates and states. Use [numpy.tanh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html) and [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) and the provided functions `sigmoid` and `softmax`.\n",
    "2. Compute the prediction $y^{\\langle t \\rangle}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JU3tUxvmELGJ"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1/(1 + np.exp(-x))\n",
    "\n",
    "def lstm_cell(x, h, c, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by):\n",
    "    \"\"\"\n",
    "    Single step of the LSTM cell.\n",
    "\n",
    "    Arguments:\n",
    "        x: your input data at timestep \"t\", numpy array of shape (n_x,m).\n",
    "        h: hidden state at timestep \"t-1\", numpy array of shape (n_h,m).\n",
    "        c: memory state at timestep \"t-1\", numpy array of shape (n_h,m).\n",
    "        Wf: weight matrix of the forget gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bf: bias of the forget gate, numpy array of shape (n_h,1).\n",
    "        Wi: weight matrix of the input gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bi: bias of the input gate, numpy array of shape (n_h,1).\n",
    "        Wc: weight matrix of the first \"tanh\", numpy array of shape (n_h,n_h + n_x).\n",
    "        bc: bias of the first \"tanh\", numpy array of shape (n_h,1).\n",
    "        Wo: weight matrix of the output gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bo: bias of the output gate, numpy array of shape (n_h,1).\n",
    "        Wy: weight matrix relating the hidden state to the output, numpy array of shape (n_y,n_h).\n",
    "        by: bias relating the hidden state to the output, numpy array of shape (n_y,1).                    \n",
    "    Returns:\n",
    "        h_next: next hidden state, of shape (n_h,m).\n",
    "        c_next: next memory state, of shape (n_h,m).\n",
    "        yt_pred: prediction at timestep \"t\", numpy array of shape (n_y,m).\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve dimensions from shapes of x and Wy and concatenate h and x\n",
    "    n_x, m = x.shape\n",
    "    n_y, n_h = Wy.shape\n",
    "    concat = np.concatenate((h, x))\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    # Compute values for f, i, cct, c, o, h using the formulas\n",
    "    f = ...\n",
    "    i = ...\n",
    "    cct = ...\n",
    "    c = ...\n",
    "    o = ...\n",
    "    h = ...\n",
    "\n",
    "    # Compute prediction of the LSTM cell\n",
    "    y_pred = ...\n",
    "    ### ^^^^^^^^^^^^^^ ###\n",
    "\n",
    "    return h, c, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9ssBEoxELGN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10)\n",
    "h = np.random.randn(5, 10)\n",
    "c = np.random.randn(5, 10)\n",
    "Wf = np.random.randn(5, 5 + 3)\n",
    "bf = np.random.randn(5, 1)\n",
    "Wi = np.random.randn(5, 5 + 3)\n",
    "bi = np.random.randn(5, 1)\n",
    "Wo = np.random.randn(5, 5 + 3)\n",
    "bo = np.random.randn(5, 1)\n",
    "Wc = np.random.randn(5, 5 + 3)\n",
    "bc = np.random.randn(5, 1)\n",
    "Wy = np.random.randn(2, 5)\n",
    "by = np.random.randn(2, 1)\n",
    "h, c, y_pred = lstm_cell(x, h, c, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by)\n",
    "\n",
    "np.testing.assert_equal(h.shape, (5, 10))\n",
    "expected_h4 = np.array([-0.66408471, 0.0036921, 0.02088357, 0.22834167, -0.85575339, 0.00138482, 0.76566531, 0.34631421, -0.00215674, 0.43827275])\n",
    "np.testing.assert_allclose(h[4], expected_h4, rtol=1e-2)\n",
    "np.testing.assert_equal(c.shape, (5, 10))\n",
    "expected_c2 = np.array([0.63267805, 1.00570849, 0.35504474, 0.20690913, -1.64566718, 0.11832942, 0.76449811, -0.0981561, -0.74348425, -0.26810932])\n",
    "np.testing.assert_allclose(c[2], expected_c2, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10))\n",
    "expected_y_pred1 = np.array([0.0283805, 0.00706782, 0.00753306, 0.01091349, 0.02806865, 0.00955958, 0.00444923, 0.01152125, 0.01426974, 0.01237766])\n",
    "np.testing.assert_allclose(y_pred[1], expected_y_pred1, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tb-4WWn4ELGQ"
   },
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - LSTM Forward Propagation\n",
    "\n",
    "Now it is possible to iterate over the LSTM cell using a for loop to process a sequence of $T_x$ inputs.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "For each time step:\n",
    "1. From the 3D tensor $x$, get a 2D slice $x^{\\langle t \\rangle}$ at time step $t$.\n",
    "2. Call the `lstm_cell` function to get the hidden state, cell state and prediction.\n",
    "3. Store the hidden state, cell state and prediction (the 2D tensors) inside the 3D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XMmJrPSdELGQ"
   },
   "outputs": [],
   "source": [
    "def lstm_forward(x, h, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by):\n",
    "    \"\"\"\n",
    "    Forward propagation of the LSTM network.\n",
    "\n",
    "    Arguments:\n",
    "        x: input data for every time step, of shape (n_x,m,T_x).\n",
    "        h: initial hidden state, of shape (n_h,m).\n",
    "        Wf: weight matrix of the forget gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bf: bias of the forget gate, numpy array of shape (n_h,1).\n",
    "        Wi: weight matrix of the input gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bi: bias of the input gate, numpy array of shape (n_h,1).\n",
    "        Wc: weight matrix of the first \"tanh\", numpy array of shape (n_h,n_h + n_x).\n",
    "        bc: bias of the first \"tanh\", numpy array of shape (n_h,1).\n",
    "        Wo: weight matrix of the output gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bo: bias of the output gate, numpy array of shape (n_h,1).\n",
    "        Wy: weight matrix relating the hidden state to the output, numpy array of shape (n_y,n_h).\n",
    "        by: bias relating the hidden state to the output, numpy array of shape (n_y,1).\n",
    "    Returns:\n",
    "        a: hidden states for every time step, numpy array of shape (n_h,m,T_x).\n",
    "        y: predictions for every time step, numpy array of shape (n_y,m,T_x).\n",
    "        c: cell state, numpy array of shape (n_h,m,T_x).\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve dimensions from shapes of x and Wy and initialize h, c and y\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_h = Wy.shape\n",
    "    ht = h\n",
    "    ct = np.zeros((n_h, m))\n",
    "    h = np.zeros((n_h, m, T_x))\n",
    "    c = np.zeros((n_h, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # loop over time steps\n",
    "    for t in range(T_x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n",
    "        xt = ...\n",
    "        # Update next hidden state and next memory state and compute the prediction with lstm_cell\n",
    "        ht, ct, yt_pred = ...\n",
    "        # Save the value of the new \"next\" hidden state in h\n",
    "        h[:,:,t] = ...\n",
    "        # Save the value of the next cell state in c\n",
    "        c[:,:,t]  = ...\n",
    "        # Save the value of the prediction in y_pred\n",
    "        y_pred[:,:,t] = ...\n",
    "        ### ^^^^^^^^^^^^^^ ###\n",
    "    \n",
    "    return h, c, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JehC5gwdELGS"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10, 7)\n",
    "h = np.random.randn(5, 10)\n",
    "Wf = np.random.randn(5, 5 + 3)\n",
    "bf = np.random.randn(5, 1)\n",
    "Wi = np.random.randn(5, 5 + 3)\n",
    "bi= np.random.randn(5, 1)\n",
    "Wo = np.random.randn(5, 5 + 3)\n",
    "bo = np.random.randn(5, 1)\n",
    "Wc = np.random.randn(5, 5 + 3)\n",
    "bc = np.random.randn(5, 1)\n",
    "Wy = np.random.randn(2, 5)\n",
    "by = np.random.randn(2, 1)\n",
    "h, c, y_pred = lstm_forward(x, h, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by)\n",
    "\n",
    "np.testing.assert_equal(h.shape, (5, 10, 7))\n",
    "expected_h436 = 0.17211776753291672\n",
    "np.testing.assert_allclose(h[4][3][6], expected_h436, rtol=1e-2)\n",
    "np.testing.assert_equal(c.shape, (5, 10, 7))\n",
    "expected_c121 = -0.8555449167181981\n",
    "np.testing.assert_allclose(c[1][2][1], expected_c121, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10, 7))\n",
    "expected_y_pred143 = 0.10837052997887556\n",
    "np.testing.assert_allclose(y_pred[1][4][3], expected_y_pred143, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's build a new classifier, this time for the IMDB dataset, using a Transformer Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "X_train = keras.utils.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = keras.utils.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using keras, implement a transformer block and a token and position embedding as layers, and use them to build a classifier. Train it for 1 epoch with Adam on the training partition while using the test partition to calculate the validation loss and accuracy at every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "...\n",
    "### ^^^^^^^^^^^^^^ ###"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "DLSC5W1-A1"
   ]
  },
  "kernelspec": {
   "display_name": "Python [conda env:keras3all]",
   "language": "python",
   "name": "conda-env-keras3all-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
