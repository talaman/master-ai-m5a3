{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyYaroWfELFo"
   },
   "source": [
    "# Programming Assignment\n",
    "\n",
    "**Warning**:\n",
    "\n",
    "Please modify only the code that is between\n",
    "```Python\n",
    "### YOUR CODE HERE ###\n",
    "```\n",
    "and\n",
    "```Python\n",
    "### ^^^^^^^^^^^^^^ ###\n",
    "```\n",
    "Some of the cells to be completed by the student are followed by a \"check\" cell that performs some basic unit tests. Run them after completing the code to check your results. Their only purpose is to help the student and will not be used to evaluate the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/vscode/.local/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: keras in /home/vscode/.local/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: tensorflow[and-cuda] in /home/vscode/.local/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/vscode/.local/lib/python3.12/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/vscode/.local/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/vscode/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/vscode/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py in /home/vscode/.local/lib/python3.12/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in /home/vscode/.local/lib/python3.12/site-packages (from keras) (13.9.3)\n",
      "Requirement already satisfied: namex in /home/vscode/.local/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/vscode/.local/lib/python3.12/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in /home/vscode/.local/lib/python3.12/site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in /home/vscode/.local/lib/python3.12/site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: packaging in /home/vscode/.local/lib/python3.12/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.18.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.5.3.2 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.5.82 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.5.82 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.5.82 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.5.82 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.3.0.75 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (9.3.0.75)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.3.61 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (11.2.3.61)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.6.82 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (10.3.6.82)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.3.83 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (11.6.3.83)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.1.3 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.5.82 in /home/vscode/.local/lib/python3.12/site-packages (from tensorflow[and-cuda]) (12.5.82)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.43.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vscode/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/vscode/.local/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/vscode/.local/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/vscode/.local/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.0.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/vscode/.local/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vscode/.local/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/vscode/.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/vscode/.local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow[and-cuda]) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn keras tensorflow[and-cuda]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a basic BoW model for a text classification problem with the [20 newsgroups](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "X, y = fetch_20newsgroups(return_X_y=True, subset=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn, create a pipeline consisting of a TF-IDF vectorization with 1-, 2- and 3-grams, and a Random Forest with 100 trees of max depth of 4. Evaluate it's f1-macro score with a 3-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-macro: 0.4785009818887566\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 3))),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=4))\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "f1_macro = cross_val_score(pipeline, X, y, cv=cv, scoring='f1_macro')\n",
    "\n",
    "print(f'f1-macro: {f1_macro.mean()}')\n",
    "### ^^^^^^^^^^^^^^ ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Notation\n",
    "\n",
    "- Subscript $n$ denotes the $n^{th}$ sample. \n",
    "\n",
    "- Superscript $[l]$ denotes the $l^{th}$ layer. \n",
    "\n",
    "- Superscript $\\langle t \\rangle$ denotes the $t^{th}$ time step. \n",
    "    \n",
    "- Subscript $(d)$ denotes the $d^{th}$ entry of a vector.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "$z^{[2]<3>}_{1(4)}$ denotes the activation of the 1st training sample, [2]nd layer, <3>rd time step, and (4)th entry in the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "03kGDM_sELFv"
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Recurrent Neural Network (RNN)\n",
    "\n",
    "### Input $x$\n",
    "\n",
    "* A training sample $x_n$ consists of multiple time steps $T_{x_n}$. Let's assume $T_x$ be the number of timesteps in the longest $x_n$.\n",
    "* A time step of an input sample, $x^{\\langle t \\rangle }_n$, is a one-dimensional input vector of size $n_x$. For example, a 1000-word vocabulary/dictionary would be encoded as a collection of one-hot vectors of size $n_x=D$ of 1000, and word $x_n^{\\langle t \\rangle}$ would have shape (1000,).\n",
    "* Let's use mini-batches of size $m$ of $x_n$ samples to benefit from vectorization. The shape of one such batch will be $(n_x,m,T_x)$. For example, if mini-batches of size 10 are used, the resulting 3D tensor will be (5000,20,10)-shaped.\n",
    "* For each time step $t$, a 2D slice $x^{\\langle t \\rangle}$ of shape $(n_x,m)$ will be fed into the network.\n",
    "\n",
    "### Hidden state $z$\n",
    "\n",
    "* The shape of a mini-batch of hidden states is $(n_z,m,T_x)$ including the time step dimension, with $n_z$ the number of hidden units.\n",
    "* $z^{\\langle t \\rangle}$ is a 2D slice at timestep $t$ of this minibatch, and has a shape of $(n_z, m)$.\n",
    "\n",
    "### Output $\\hat{y}$\n",
    "* $\\hat{y}$ is a 3D tensor of shape $(n_y,m,T_y)$, with $n_{y}$ the number of units in the vector representing the prediction and $T_{y}$ the number of time steps in the prediction. Lets assume $T_y = T_x$.\n",
    "* $\\hat{y}^{\\langle t \\rangle}$ is a 2D slice for time step $t$ and has a shape of $(n_{y}, m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oXWAKeTELF0"
   },
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - RNN Cell\n",
    "\n",
    "Let's start by implementing the computations for a single time step of the RNN cell.\n",
    "\n",
    "**Instructions**:\n",
    "1. Compute the hidden state: $z^{\\langle t \\rangle} = \\tanh(W_z z^{\\langle t-1 \\rangle} + W_x x^{\\langle t \\rangle} + b)$. Use [numpy.tanh](https://numpy.org/devdocs/reference/generated/numpy.tanh.html) and [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html).\n",
    "2. Use hidden state $z^{\\langle t \\rangle}$ to compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_y z^{\\langle t \\rangle} + b_y)$. Use the provided `softmax` function.\n",
    "4. Return $z^{\\langle t \\rangle}$ , $\\hat{y}^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxI-F0HWELF1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "  expon = np.exp(x)\n",
    "  return expon/np.sum(expon)\n",
    "\n",
    "def rnn_cell(x, z, Wx, Wy, Wz, b, by):\n",
    "    \"\"\"\n",
    "    Single step of the RNN cell.\n",
    "\n",
    "    Arguments:\n",
    "        x: input data at timestep \"t\", numpy array of shape (n_x,m).\n",
    "        z: hidden state at timestep \"t-1\", numpy array of shape (n_z,m)\n",
    "        Wx: weight matrix multiplying the input, numpy array of shape (n_z,n_x)\n",
    "        Wy: weight matrix relating the hidden-state to the output, numpy array of shape (n_y,n_z)\n",
    "        Wz: weight matrix multiplying the hidden state, numpy array of shape (n_z,n_z)\n",
    "        b: bias, numpy array of shape (n_z,1)\n",
    "        by: bias relating the hidden-state to the output, numpy array of shape (n_y,1)\n",
    "    Returns:\n",
    "        z: hidden state at timestep \"t\", of shape (n_z,m)\n",
    "        y_pred: prediction at timestep \"t\", numpy array of shape (n_y,m)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    z = np.tanh(np.dot(Wz, z) + np.dot(Wx, x) + b)\n",
    "    y_pred = softmax(np.dot(Wy, z) + by)\n",
    "    ### ^^^^^^^^^^^^^^ ###\n",
    "    return z, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V03ZGazVELF4"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10)\n",
    "z = np.random.randn(5, 10)\n",
    "Wx = np.random.randn(5, 3)\n",
    "Wz = np.random.randn(5, 5)\n",
    "Wy = np.random.randn(2, 5)\n",
    "b = np.random.randn(5, 1)\n",
    "by = np.random.randn(2, 1)\n",
    "z, y_pred = rnn_cell(x, z, Wx, Wy, Wz, b, by)\n",
    "\n",
    "np.testing.assert_equal(z.shape, (5, 10))\n",
    "expected_z4 = np.array([-0.97683053, 0.99993218, 0.9999085, 0.99720415, 0.96532437, 0.24615069, 0.89433073, 0.88371261, 0.40821569, -0.99446565])\n",
    "np.testing.assert_allclose(z[4], expected_z4, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10))\n",
    "expected_y_pred1 = np.array([0.00402888, 0.03824577, 0.03569951, 0.07060504, 0.01322519, 0.03128045, 0.02207267, 0.25945201, 0.03835635, 0.00209625])\n",
    "np.testing.assert_allclose(y_pred[1], expected_y_pred1, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rjz378-tELF7"
   },
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - RNN Forward Propagation \n",
    "\n",
    "A recurrent neural network can now be implemented as the repeated use of a cell. If the input sequence is $T_x$ time steps long, the RNN cell will be called $T_x$ times.\n",
    "\n",
    "- The cell takes two inputs at each time step:\n",
    "    - $z^{\\langle t-1 \\rangle}$: the hidden state from the previous time step\n",
    "    - $x^{\\langle t \\rangle}$: the current time step's input data\n",
    "- The cell produces two outputs at each time step:\n",
    "    - $z^{\\langle t \\rangle}$: the hidden state at this time step\n",
    "    - $\\hat{y}^{\\langle t \\rangle}$: the prediction for this time step\n",
    "- The weights and biases $(W_z, W_x, b)$ are re-used each time step \n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "At each time step $t$:\n",
    "1. Get $x^{\\langle t \\rangle}$, the 2D slice of $x$ at time step $t$.\n",
    "2. Update the 2D hidden state $z^{\\langle t \\rangle}$ and the prediction $\\hat{y}^{\\langle t \\rangle}$ using `rnn_cell`.\n",
    "3. Store the 2D hidden state in the 3D tensor $z$, at the $t^{th}$ position.\n",
    "4. Store the 2D $\\hat{y}^{\\langle t \\rangle}$ prediction in the 3D tensor $\\hat{y}$ at the $t^{th}$ position.\n",
    "5. Return the 3D tensors $z$ and $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VmeprGJpELF9"
   },
   "outputs": [],
   "source": [
    "def rnn_forward(x, z, Wx, Wy, Wz, b, by):\n",
    "    \"\"\"\n",
    "    Forward propagation of the RNN.\n",
    "\n",
    "    Arguments:\n",
    "        x: input data for every time-step, of shape (n_x,m,T_x).\n",
    "        z: initial hidden state, of shape (n_z,m)\n",
    "        Wx: weight matrix multiplying the input, numpy array of shape (n_z,n_x)\n",
    "        Wy: weight matrix relating the hidden-state to the output, numpy array of shape (n_y,n_z)\n",
    "        Wz: weight matrix multiplying the hidden state, numpy array of shape (n_z,n_z)\n",
    "        b: bias numpy array of shape (n_z,1)\n",
    "        by: bias relating the hidden-state to the output, numpy array of shape (n_y,1)\n",
    "    Returns:\n",
    "        z: hidden states for every time-step, numpy array of shape (n_z,m,T_x)\n",
    "        y_pred: predictions for every time-step, numpy array of shape (n_y,m,T_x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions and initialize z, y_pred and zt\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_z = Wy.shape\n",
    "    zt = z\n",
    "    z = np.zeros((n_z, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # loop over time steps\n",
    "    for t in range(T_x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Update the hidden state and compute the prediction with rnn_cell\n",
    "        zt, yt_pred = rnn_cell(x[:, :, t], zt, Wx, Wy, Wz, b, by)\n",
    "        # Save the value of the hidden state at time step t in z\n",
    "        z[:, :, t] = zt\n",
    "        # Save the value of the prediction at time step t in y_pred\n",
    "        y_pred[:, :, t] = yt_pred\n",
    "        ### ^^^^^^^^^^^^^^ ###\n",
    "    \n",
    "    return z, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEPrd77rELF_"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10, 4)\n",
    "z = np.random.randn(5, 10)\n",
    "Wz = np.random.randn(5, 5)\n",
    "Wx = np.random.randn(5, 3)\n",
    "Wy = np.random.randn(2, 5)\n",
    "b = np.random.randn(5, 1)\n",
    "by = np.random.randn(2, 1)\n",
    "z, y_pred = rnn_forward(x, z, Wx, Wy, Wz, b, by)\n",
    "\n",
    "np.testing.assert_equal(z.shape, (5, 10, 4))\n",
    "expected_z4 = np.array([-0.99999375, 0.77911235, -0.99861469, -0.99833267])\n",
    "np.testing.assert_allclose(z[4][1], expected_z4, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10, 4))\n",
    "expected_y_pred1 = np.array([0.01644489, 0.0328586, 0.0014877, 0.02851197])\n",
    "np.testing.assert_allclose(y_pred[1][3], expected_y_pred1, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2QbsWFzELGD"
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Long Short-Term Memory (LSTM)\n",
    "\n",
    "The RNN works best when each output $\\hat{y}^{\\langle t \\rangle}$ can be estimated using \"local\" context, but suffers from vanishing gradient and cannot remember information for many time steps. LSTM can cope with vanishing gradient much better.\n",
    "\n",
    "Let's begin by implementing the LSTM cell for a single time step. Then, it will be possible to call it from a loop to have it process an input with $T_x$ time steps.\n",
    "\n",
    "### Gates and states\n",
    "\n",
    "#### Forget gate $f$\n",
    "\n",
    "The forget gate is a tensor containing values between 0 and 1.\n",
    "    * If a unit in the forget gate has a value close to 0, the LSTM will \"forget\" the stored previous state in the corresponding unit.\n",
    "    * If a unit in the forget gate has a value close to 1, the LSTM will \"remember\" the the stored previous state in the corresponding unit.\n",
    "\n",
    "$$f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{h}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)$$\n",
    "\n",
    "#### Candidate cell value $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$\n",
    "* The candidate cell value is a tensor containing information from the current time step that may be stored in the current cell state $\\mathbf{c}^{\\langle t \\rangle}$.\n",
    "* The parts of the candidate cell value that get passed on depend on the input gate.\n",
    "* The candidate cell value is a tensor containing values that range from -1 to 1.\n",
    "\n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{h}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right)$$\n",
    "\n",
    "#### Input gate $i$\n",
    "\n",
    "* The input gate decides what entries of the candidate cell value $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ to add to the cell state $c^{\\langle t \\rangle}$.\n",
    "* The update gate is a tensor containing values between 0 and 1.\n",
    "    * When a unit in the update gate is close to 1, it allows the value of the candidate $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ to be passed onto the hidden state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "    * When a unit in the update gate is close to 0, it prevents the corresponding value in the candidate from being passed onto the hidden state.\n",
    "\n",
    "$$i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[h^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\\tag{2}$$\n",
    "\n",
    "#### Cell state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "\n",
    "* The cell state is the \"memory\" passed to future time steps.\n",
    "* The new cell state $\\mathbf{c}^{\\langle t \\rangle}$ is a combination of the previous cell state and the candidate value.\n",
    "\n",
    "$$\\mathbf{c}^{\\langle t \\rangle} = f^{\\langle t \\rangle}*\\mathbf{c}^{\\langle t-1 \\rangle} + i^{\\langle t \\rangle}*\\mathbf{\\tilde{c}}^{\\langle t \\rangle}$$\n",
    "\n",
    "#### Output gate $o$\n",
    "\n",
    "* The output gate decides what gets sent as the output (prediction) of the time step.\n",
    "* The output gate contains values that range from 0 to 1.\n",
    "\n",
    "$$o^{\\langle t \\rangle}= \\sigma(\\mathbf{W}_o[\\mathbf{h}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})$$ \n",
    "\n",
    "\n",
    "#### Hidden state $\\mathbf{h}^{\\langle t \\rangle}$\n",
    "\n",
    "* The hidden state gets passed to the LSTM cell's next time step.\n",
    "* It is used to determine the three gates ($f, i, o$) of the next time step.\n",
    "* The hidden state is also used for the prediction $y^{\\langle t \\rangle}$.\n",
    "\n",
    "$$ \\mathbf{h}^{\\langle t \\rangle} = o^{\\langle t \\rangle}*\\tanh(\\mathbf{c}^{\\langle t \\rangle})$$\n",
    "\n",
    "\n",
    "#### Prediction $\\mathbf{y}^{\\langle t \\rangle}_{pred}$\n",
    "\n",
    "* The output or prediction.\n",
    "\n",
    "If the problem is multiclass classification (therefore with a softmax activation at the output), for example, the equation would be:\n",
    "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{h}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G49sqmnoELGI"
   },
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - LSTM Cell\n",
    "\n",
    "Lets implement the LSTM cell.\n",
    "\n",
    "**Instructions**:\n",
    "1. Compute all formulas for the gates and states. Use [numpy.tanh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html) and [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) and the provided functions `sigmoid` and `softmax`.\n",
    "2. Compute the prediction $y^{\\langle t \\rangle}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JU3tUxvmELGJ"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1/(1 + np.exp(-x))\n",
    "\n",
    "def lstm_cell(x, h, c, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by):\n",
    "    \"\"\"\n",
    "    Single step of the LSTM cell.\n",
    "\n",
    "    Arguments:\n",
    "        x: your input data at timestep \"t\", numpy array of shape (n_x,m).\n",
    "        h: hidden state at timestep \"t-1\", numpy array of shape (n_h,m).\n",
    "        c: memory state at timestep \"t-1\", numpy array of shape (n_h,m).\n",
    "        Wf: weight matrix of the forget gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bf: bias of the forget gate, numpy array of shape (n_h,1).\n",
    "        Wi: weight matrix of the input gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bi: bias of the input gate, numpy array of shape (n_h,1).\n",
    "        Wc: weight matrix of the first \"tanh\", numpy array of shape (n_h,n_h + n_x).\n",
    "        bc: bias of the first \"tanh\", numpy array of shape (n_h,1).\n",
    "        Wo: weight matrix of the output gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bo: bias of the output gate, numpy array of shape (n_h,1).\n",
    "        Wy: weight matrix relating the hidden state to the output, numpy array of shape (n_y,n_h).\n",
    "        by: bias relating the hidden state to the output, numpy array of shape (n_y,1).                    \n",
    "    Returns:\n",
    "        h_next: next hidden state, of shape (n_h,m).\n",
    "        c_next: next memory state, of shape (n_h,m).\n",
    "        yt_pred: prediction at timestep \"t\", numpy array of shape (n_y,m).\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve dimensions from shapes of x and Wy and concatenate h and x\n",
    "    n_x, m = x.shape\n",
    "    n_y, n_h = Wy.shape\n",
    "    concat = np.concatenate((h, x))\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    # Compute values for f, i, cct, c, o, h using the formulas\n",
    "    f = sigmoid(np.dot(Wf, concat) + bf)\n",
    "    i = sigmoid(np.dot(Wi, concat) + bi)\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)\n",
    "    c = f * c + i * cct\n",
    "    o = sigmoid(np.dot(Wo, concat) + bo)\n",
    "    h = o * np.tanh(c)\n",
    "\n",
    "    # Compute prediction of the LSTM cell\n",
    "    y_pred = softmax(np.dot(Wy, h) + by)\n",
    "    ### ^^^^^^^^^^^^^^ ###\n",
    "\n",
    "    return h, c, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9ssBEoxELGN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10)\n",
    "h = np.random.randn(5, 10)\n",
    "c = np.random.randn(5, 10)\n",
    "Wf = np.random.randn(5, 5 + 3)\n",
    "bf = np.random.randn(5, 1)\n",
    "Wi = np.random.randn(5, 5 + 3)\n",
    "bi = np.random.randn(5, 1)\n",
    "Wo = np.random.randn(5, 5 + 3)\n",
    "bo = np.random.randn(5, 1)\n",
    "Wc = np.random.randn(5, 5 + 3)\n",
    "bc = np.random.randn(5, 1)\n",
    "Wy = np.random.randn(2, 5)\n",
    "by = np.random.randn(2, 1)\n",
    "h, c, y_pred = lstm_cell(x, h, c, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by)\n",
    "\n",
    "np.testing.assert_equal(h.shape, (5, 10))\n",
    "expected_h4 = np.array([-0.66408471, 0.0036921, 0.02088357, 0.22834167, -0.85575339, 0.00138482, 0.76566531, 0.34631421, -0.00215674, 0.43827275])\n",
    "np.testing.assert_allclose(h[4], expected_h4, rtol=1e-2)\n",
    "np.testing.assert_equal(c.shape, (5, 10))\n",
    "expected_c2 = np.array([0.63267805, 1.00570849, 0.35504474, 0.20690913, -1.64566718, 0.11832942, 0.76449811, -0.0981561, -0.74348425, -0.26810932])\n",
    "np.testing.assert_allclose(c[2], expected_c2, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10))\n",
    "expected_y_pred1 = np.array([0.0283805, 0.00706782, 0.00753306, 0.01091349, 0.02806865, 0.00955958, 0.00444923, 0.01152125, 0.01426974, 0.01237766])\n",
    "np.testing.assert_allclose(y_pred[1], expected_y_pred1, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tb-4WWn4ELGQ"
   },
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - LSTM Forward Propagation\n",
    "\n",
    "Now it is possible to iterate over the LSTM cell using a for loop to process a sequence of $T_x$ inputs.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "For each time step:\n",
    "1. From the 3D tensor $x$, get a 2D slice $x^{\\langle t \\rangle}$ at time step $t$.\n",
    "2. Call the `lstm_cell` function to get the hidden state, cell state and prediction.\n",
    "3. Store the hidden state, cell state and prediction (the 2D tensors) inside the 3D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XMmJrPSdELGQ"
   },
   "outputs": [],
   "source": [
    "def lstm_forward(x, h, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by):\n",
    "    \"\"\"\n",
    "    Forward propagation of the LSTM network.\n",
    "\n",
    "    Arguments:\n",
    "        x: input data for every time step, of shape (n_x,m,T_x).\n",
    "        h: initial hidden state, of shape (n_h,m).\n",
    "        Wf: weight matrix of the forget gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bf: bias of the forget gate, numpy array of shape (n_h,1).\n",
    "        Wi: weight matrix of the input gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bi: bias of the input gate, numpy array of shape (n_h,1).\n",
    "        Wc: weight matrix of the first \"tanh\", numpy array of shape (n_h,n_h + n_x).\n",
    "        bc: bias of the first \"tanh\", numpy array of shape (n_h,1).\n",
    "        Wo: weight matrix of the output gate, numpy array of shape (n_h,n_h + n_x).\n",
    "        bo: bias of the output gate, numpy array of shape (n_h,1).\n",
    "        Wy: weight matrix relating the hidden state to the output, numpy array of shape (n_y,n_h).\n",
    "        by: bias relating the hidden state to the output, numpy array of shape (n_y,1).\n",
    "    Returns:\n",
    "        a: hidden states for every time step, numpy array of shape (n_h,m,T_x).\n",
    "        y: predictions for every time step, numpy array of shape (n_y,m,T_x).\n",
    "        c: cell state, numpy array of shape (n_h,m,T_x).\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve dimensions from shapes of x and Wy and initialize h, c and y\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_h = Wy.shape\n",
    "    ht = h\n",
    "    ct = np.zeros((n_h, m))\n",
    "    h = np.zeros((n_h, m, T_x))\n",
    "    c = np.zeros((n_h, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # loop over time steps\n",
    "    for t in range(T_x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n",
    "        xt = x[:, :, t]\n",
    "        # Update next hidden state and next memory state and compute the prediction with lstm_cell\n",
    "        ht, ct, yt_pred = lstm_cell(xt, ht, ct, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by)\n",
    "        # Save the value of the new \"next\" hidden state in h\n",
    "        h[:, :, t] = ht\n",
    "        # Save the value of the next cell state in c\n",
    "        c[:, :, t] = ct\n",
    "        # Save the value of the prediction in y_pred\n",
    "        y_pred[:, :, t] = yt_pred\n",
    "        ### ^^^^^^^^^^^^^^ ###\n",
    "    \n",
    "    return h, c, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JehC5gwdELGS"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3, 10, 7)\n",
    "h = np.random.randn(5, 10)\n",
    "Wf = np.random.randn(5, 5 + 3)\n",
    "bf = np.random.randn(5, 1)\n",
    "Wi = np.random.randn(5, 5 + 3)\n",
    "bi= np.random.randn(5, 1)\n",
    "Wo = np.random.randn(5, 5 + 3)\n",
    "bo = np.random.randn(5, 1)\n",
    "Wc = np.random.randn(5, 5 + 3)\n",
    "bc = np.random.randn(5, 1)\n",
    "Wy = np.random.randn(2, 5)\n",
    "by = np.random.randn(2, 1)\n",
    "h, c, y_pred = lstm_forward(x, h, Wf, bf, Wi, bi, Wc, bc, Wo, bo, Wy, by)\n",
    "\n",
    "np.testing.assert_equal(h.shape, (5, 10, 7))\n",
    "expected_h436 = 0.17211776753291672\n",
    "np.testing.assert_allclose(h[4][3][6], expected_h436, rtol=1e-2)\n",
    "np.testing.assert_equal(c.shape, (5, 10, 7))\n",
    "expected_c121 = -0.8555449167181981\n",
    "np.testing.assert_allclose(c[1][2][1], expected_c121, rtol=1e-2)\n",
    "np.testing.assert_equal(y_pred.shape, (2, 10, 7))\n",
    "expected_y_pred143 = 0.10837052997887556\n",
    "np.testing.assert_allclose(y_pred[1][4][3], expected_y_pred143, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's build a new classifier, this time for the IMDB dataset, using a Transformer Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 11:57:35.092623: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-05 11:57:35.236603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730807855.289602    1341 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730807855.303550    1341 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-05 11:57:35.426968: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "X_train = keras.utils.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = keras.utils.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using keras, implement a transformer block and a token and position embedding as layers, and use them to build a classifier. Train it for 1 epoch with Adam on the training partition while using the test partition to calculate the validation loss and accuracy at every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730807860.348871    1341 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730807866.502692    1642 service.cc:148] XLA service 0x7f13e401cf80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1730807866.503012    1642 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2024-11-05 11:57:46.657385: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1730807867.281481    1642 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-05 11:57:48.655929: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_145', 176 bytes spill stores, 208 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:48.709522: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_145', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:48.851582: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_91', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:49.057812: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:49.202901: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 160 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:49.206597: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 996 bytes spill stores, 768 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:49.692254: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:49.693522: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:49.778483: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_145', 708 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:49.981725: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 716 bytes spill stores, 740 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:50.020948: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:50.059378: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_156', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:50.089223: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 40 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:50.239976: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:50.289935: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 388 bytes spill stores, 356 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:50.382888: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_146', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:50.493830: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 176 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:51.205857: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9710', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:51.533576: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:51.535292: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_156', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:51.820301: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_155', 788 bytes spill stores, 812 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:51.894863: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_156', 988 bytes spill stores, 760 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:51.896735: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9710', 20 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:52.022617: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9851', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:52.476991: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9710', 1056 bytes spill stores, 996 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:52.549152: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28_0', 332 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:52.574962: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9710', 744 bytes spill stores, 768 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:52.622628: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 88 bytes spill stores, 88 bytes spill loads\n",
      "\n",
      "2024-11-05 11:57:58.353199: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'copy_fusion', 608 bytes spill stores, 608 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 52 bytes spill stores, 52 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_add_reduce_select_fusion_2', 44 bytes spill stores, 44 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_add_reduce_select_fusion_1', 44 bytes spill stores, 44 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_add_reduce_select_fusion', 96 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "I0000 00:00:1730807878.413840    1642 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5008 - loss: 0.7837"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 11:58:24.154902: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 80 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:24.216570: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:24.229584: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 208 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:24.510092: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:24.526317: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 40 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:24.750947: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:24.775005: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:24.797400: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 996 bytes spill stores, 768 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:24.806239: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:25.057651: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:25.224734: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 160 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:25.268180: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 176 bytes spill stores, 208 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:25.405024: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:25.413916: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_153', 708 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:25.491918: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_161', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:25.640414: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:25.912072: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 384 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:26.005855: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_164', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:26.209160: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_161', 716 bytes spill stores, 740 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:26.460303: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:26.469406: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_164', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:26.793699: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_161', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:26.869310: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 740 bytes spill stores, 764 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:26.877017: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_163', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:27.140692: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28_0', 2112 bytes spill stores, 2880 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:27.262518: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_163', 176 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:27.412263: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:27.570533: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_163', 788 bytes spill stores, 812 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:27.960520: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:28.135806: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_164', 988 bytes spill stores, 760 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:28.293054: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_57', 1176 bytes spill stores, 1160 bytes spill loads\n",
      "\n",
      "2024-11-05 11:58:34.653638: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_10', 384 bytes spill stores, 384 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 8 bytes spill stores, 8 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_add_multiply_reduce_select_fusion', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 59ms/step - accuracy: 0.5008 - loss: 0.7835 - val_accuracy: 0.5000 - val_loss: 0.6982\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.4974 - loss: 0.6987 - val_accuracy: 0.5000 - val_loss: 0.6967\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.4931 - loss: 0.6975 - val_accuracy: 0.5000 - val_loss: 0.6933\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.5018 - loss: 0.6951 - val_accuracy: 0.5000 - val_loss: 0.6977\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.5005 - loss: 0.6958 - val_accuracy: 0.5000 - val_loss: 0.6958\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.4922 - loss: 0.6955 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.5030 - loss: 0.6943 - val_accuracy: 0.5000 - val_loss: 0.6941\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.4978 - loss: 0.6939 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.4989 - loss: 0.6939 - val_accuracy: 0.5000 - val_loss: 0.6934\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.4944 - loss: 0.6937 - val_accuracy: 0.5000 - val_loss: 0.6934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f1546a663c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, Dropout\n",
    "from keras.layers import LayerNormalization, MultiHeadAttention, Add\n",
    "\n",
    "def PositionalEncoding(maxlen, model_dim):\n",
    "    def _positional_encoding(inputs):\n",
    "        _, maxlen, model_dim = inputs.shape\n",
    "        pos = np.arange(0, maxlen).reshape(-1, 1)\n",
    "        i = np.arange(0, model_dim).reshape(1, -1)\n",
    "        angle_rads = pos / np.power(10000, (2 * (i // 2) / model_dim))\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        angle_rads = angle_rads[np.newaxis, ...]\n",
    "        return inputs + angle_rads\n",
    "    return keras.layers.Lambda(_positional_encoding)\n",
    "\n",
    "def transformer_block(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = LayerNormalization()(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    # Residual and Feed Forward\n",
    "    x = Add()([inputs, x])\n",
    "    y = LayerNormalization()(x)\n",
    "    x = Dense(ff_dim, activation='relu')(y)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    return Add()([y, x])\n",
    "\n",
    "head_size = 256\n",
    "num_heads = 4\n",
    "ff_dim = 4\n",
    "dropout = 0.3\n",
    "inputs = Input(shape=(maxlen,))\n",
    "embedding = Embedding(vocab_size, 128)(inputs)\n",
    "x = embedding + PositionalEncoding(maxlen, 128)(embedding)\n",
    "for _ in range(4):\n",
    "    x = transformer_block(x, head_size, num_heads, ff_dim, dropout)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(dropout)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "### ^^^^^^^^^^^^^^ ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.5938 - loss: 0.6895"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.4928 - loss: 0.6937\n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "DLSC5W1-A1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
